{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjXSou8P+uja8DZyQ6OQRQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paveethranswam/Speech-style-transfer-and-ASR/blob/main/final_Deep_Speech_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reference: https://www.assemblyai.com/blog/end-to-end-speech-recognition-pytorch/\n",
        "# https://keras.io/examples/audio/ctc_asr/"
      ],
      "metadata": {
        "id": "yt9HRuO3HlzI"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYPHIN_hILK1",
        "outputId": "0278f9bc-3199-4afe-94f8-9950ec6e6b75"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "iSbeVX0Vj_f4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYTcOOWOkSij",
        "outputId": "4bffe7b6-5df6-4d00-b945-6a88afb4b845"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda:0 device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33U-KgQzlWHT",
        "outputId": "bd903768-73af-48cb-f3b2-52dcb01b2188"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-f00aff75-0f32-32c8-1522-f801c2f5fdac)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set torch random seed \n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "torch.backends.cudnn.enabled = False\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "EpLSmAUVlWKS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchaudio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFF37YzPlWON",
        "outputId": "d16fa9ae-7296-42e3-86be-cc89434d5fa6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.8/dist-packages (0.13.0+cu116)\n",
            "Requirement already satisfied: torch==1.13.0 in /usr/local/lib/python3.8/dist-packages (from torchaudio) (1.13.0+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.13.0->torchaudio) (4.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import IPython\n",
        "import requests\n",
        "import torchaudio\n",
        "from IPython.display import Audio, display\n",
        "from torch._C import parse_schema"
      ],
      "metadata": {
        "id": "ZLtKsxXPlcQd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/My Drive/DLS_Project/Dataset/DR-VCTK/DR-VCTK.zip\" -d \"./DR-VCTK\""
      ],
      "metadata": {
        "id": "FGGu-hGZlcTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DR_VCTK_DATASET_PATH = \"./DR-VCTK/\""
      ],
      "metadata": {
        "id": "hWOpYCN2kSlW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(DR_VCTK_DATASET_PATH, exist_ok=True)\n",
        "print(DR_VCTK_DATASET_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVfDxg9Wj_if",
        "outputId": "a5d7a3e7-93f7-4491-9d8c-b476d061fc93"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./DR-VCTK/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting precomputed transcript mapping\n",
        "import pickle\n",
        "from google.colab import files\n",
        "\n",
        "with open('./final_w2v_transcript_dictionary2.pkl', 'rb') as f:\n",
        "    speaker_utterance_final = pickle.load(f)"
      ],
      "metadata": {
        "id": "fj3JuKlplkqS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drvctk_train_dataset = torchaudio.datasets.DR_VCTK(DR_VCTK_DATASET_PATH, subset='train',download=False)"
      ],
      "metadata": {
        "id": "HqCcdWJ7lkuP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drvctk_test_dataset = torchaudio.datasets.DR_VCTK(DR_VCTK_DATASET_PATH, subset='test',download=False)"
      ],
      "metadata": {
        "id": "6NFSL4J5lkwk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(drvctk_train_dataset))\n",
        "print(len(drvctk_test_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJ011_rrlkyr",
        "outputId": "7a0d9823-fc27-485f-96b8-628fb42295eb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11572\n",
            "824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn"
      ],
      "metadata": {
        "id": "7nbfV5G3GmBO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jiwer\n",
        "from jiwer import wer\n",
        "from jiwer import cer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOCcPHnKMEs3",
        "outputId": "1c27ed1d-e3b4-4a79-92bb-2ceb443ef352"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting jiwer\n",
            "  Downloading jiwer-2.5.1-py3-none-any.whl (15 kB)\n",
            "Collecting levenshtein==0.20.2\n",
            "  Downloading Levenshtein-0.20.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 38.8 MB/s \n",
            "\u001b[?25hCollecting rapidfuzz<3.0.0,>=2.3.0\n",
            "  Downloading rapidfuzz-2.13.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 66.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, levenshtein, jiwer\n",
            "Successfully installed jiwer-2.5.1 levenshtein-0.20.2 rapidfuzz-2.13.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_map = {\"'\": 0, 'a': 2, 'b': 3, 'c': 4, 'd': 5, 'e': 6, 'f': 7, 'g': 8, 'h': 9, 'i': 10, 'j': 11, 'k': 12, 'l': 13, 'm': 14, 'n': 15, 'o': 16, 'p': 17, 'q': 18, 'r': 19, 's': 20, 't': 21, 'u': 22, 'v': 23, 'w': 24, 'x': 25, 'y': 26, 'z': 27, ' ': 1}"
      ],
      "metadata": {
        "id": "5Lb1em5A-D-S"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_char_map = {0: \"'\", 2: 'a', 3: 'b', 4: 'c', 5: 'd', 6: 'e', 7: 'f', 8: 'g', 9: 'h', 10: 'i', 11: 'j', 12: 'k', 13: 'l', 14: 'm', 15: 'n', 16: 'o', 17: 'p', 18: 'q', 19: 'r', 20: 's', 21: 't', 22: 'u', 23: 'v', 24: 'w', 25: 'x', 26: 'y', 27: 'z', 1: ' '}"
      ],
      "metadata": {
        "id": "loAPlfrF-ID7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label_to_index(text):\n",
        "    int_sequence = []\n",
        "    for c in text:\n",
        "        if c == ' ':\n",
        "            ch = char_map[' ']\n",
        "        else:\n",
        "            ch = char_map[c]\n",
        "        int_sequence.append(ch)\n",
        "    return int_sequence\n",
        "\n",
        "def index_to_label(labels):\n",
        "    string = []\n",
        "    for i in labels:\n",
        "        string.append(reverse_char_map[i])\n",
        "    return ''.join(string)"
      ],
      "metadata": {
        "id": "x6HUQNUe-XDt"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforms on input spectrograms\n",
        "\n",
        "train_audio_transforms = nn.Sequential(\n",
        "    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n",
        "    torchaudio.transforms.FrequencyMasking(freq_mask_param=15),\n",
        "    torchaudio.transforms.TimeMasking(time_mask_param=35)\n",
        ")\n",
        "\n",
        "valid_audio_transforms = torchaudio.transforms.MelSpectrogram()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWvn2AV6-wXF",
        "outputId": "9922331b-2e1c-449b-b66c-9a8b3425c73f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchaudio/functional/functional.py:571: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def processing_collate_fn(data, type1=\"train\", type2 = \"clean\"):\n",
        "\n",
        "    # returns\n",
        "    spectrograms = []\n",
        "    labels = []\n",
        "    input_lengths = []\n",
        "    label_lengths = []\n",
        "\n",
        "    for (clean_waveform, _, noisy_waveform, _, speaker_id, utterance_id, _, _) in data:\n",
        "        if type1 == 'train':\n",
        "          if type2 == 'clean':\n",
        "            spec = train_audio_transforms(clean_waveform).squeeze(0).transpose(0, 1)\n",
        "          else:\n",
        "            spec = train_audio_transforms(noisy_waveform).squeeze(0).transpose(0, 1)\n",
        "        else:\n",
        "          if type2 == 'clean':\n",
        "            spec = valid_audio_transforms(clean_waveform).squeeze(0).transpose(0, 1)\n",
        "          else:\n",
        "            spec = valid_audio_transforms(noisy_waveform).squeeze(0).transpose(0, 1)\n",
        "\n",
        "        spectrograms.append(spec)\n",
        "\n",
        "        sample_utterance = speaker_utterance_final[speaker_id][utterance_id]\n",
        "\n",
        "        sample_utterance = sample_utterance.lower()\n",
        "\n",
        "\n",
        "        res = []\n",
        "        for c_i in range(len(sample_utterance)):\n",
        "\n",
        "          if sample_utterance[c_i] == \"|\":\n",
        "            res.append(\" \") \n",
        "          else:\n",
        "            res.append(sample_utterance[c_i])\n",
        "        sample_utterance = \"\".join(res)\n",
        "\n",
        "        sample_utterance = sample_utterance.rstrip()\n",
        "\n",
        "        good = set(char_map.keys())\n",
        "        if set(sample_utterance) <= good:\n",
        "            pass\n",
        "        else:\n",
        "            print(sample_utterance)\n",
        "            print(\"Issue with Label\")\n",
        "\n",
        "  \n",
        "        label = torch.Tensor(label_to_index(sample_utterance))\n",
        "        labels.append(label)\n",
        "        input_lengths.append(spec.shape[0]//2)\n",
        "        label_lengths.append(len(label))\n",
        "\n",
        "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
        "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
        "\n",
        "    return spectrograms, labels, input_lengths, label_lengths"
      ],
      "metadata": {
        "id": "sgtjp4n6CvV9"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class CNNLayerNorm(nn.Module):\n",
        "    \"\"\"Layer normalization built for cnns input\"\"\"\n",
        "    def __init__(self, n_feats):\n",
        "        super(CNNLayerNorm, self).__init__()\n",
        "        self.layer_norm = nn.LayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x (batch, channel, feature, time)\n",
        "        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n",
        "        x = self.layer_norm(x)\n",
        "        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time) \n",
        "\n",
        "class ResidualCNN(nn.Module):\n",
        "    \"\"\"Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf\n",
        "        except with layer norm instead of batch norm\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
        "        super(ResidualCNN, self).__init__()\n",
        "\n",
        "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n",
        "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
        "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x  # (batch, channel, feature, time)\n",
        "        x = self.layer_norm1(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.cnn1(x)\n",
        "        x = self.layer_norm2(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.cnn2(x)\n",
        "        x += residual\n",
        "        return x # (batch, channel, feature, time)\n",
        "        \n",
        "class BidirectionalGRU(nn.Module):\n",
        "\n",
        "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
        "        super(BidirectionalGRU, self).__init__()\n",
        "\n",
        "        self.BiGRU = nn.GRU(\n",
        "            input_size=rnn_dim, hidden_size=hidden_size,\n",
        "            num_layers=1, batch_first=batch_first, bidirectional=True)\n",
        "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        x = F.gelu(x)\n",
        "        x, _ = self.BiGRU(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SpeechRecognitionModel(nn.Module):\n",
        "    \"\"\"Speech Recognition Model Inspired by DeepSpeech 2\"\"\"\n",
        "\n",
        "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):\n",
        "        super(SpeechRecognitionModel, self).__init__()\n",
        "        n_feats = n_feats//2\n",
        "        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3//2)  # cnn for extracting heirachal features\n",
        "\n",
        "        # n residual cnn layers with filter size of 32\n",
        "        self.rescnn_layers = nn.Sequential(*[\n",
        "            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats) \n",
        "            for _ in range(n_cnn_layers)\n",
        "        ])\n",
        "        self.fully_connected = nn.Linear(n_feats*32, rnn_dim)\n",
        "        self.birnn_layers = nn.Sequential(*[\n",
        "            BidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n",
        "                             hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n",
        "            for i in range(n_rnn_layers)\n",
        "        ])\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(rnn_dim*2, rnn_dim),  # birnn returns rnn_dim*2\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(rnn_dim, n_class)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn(x)\n",
        "        x = self.rescnn_layers(x)\n",
        "        sizes = x.size()\n",
        "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
        "        x = x.transpose(1, 2) # (batch, time, feature)\n",
        "        x = self.fully_connected(x)\n",
        "        x = self.birnn_layers(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "cVFvuOi0CvXM"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Hyperparameters\n",
        "n_epochs = 10 # max_epochs\n",
        "batch_size_train = 15 # batch size train set\n",
        "batch_size_test = 15 # batch size test set\n",
        "learning_rate = 5e-4 # learn rate\n",
        "log_interval = 1 # log every _ epoch\n",
        "\n",
        "\n",
        "n_cnn_layers = 3\n",
        "n_rnn_layers = 3\n",
        "rnn_dim = 512\n",
        "n_class = 29\n",
        "n_feats = 128\n",
        "stride = 2\n",
        "dropout = .1"
      ],
      "metadata": {
        "id": "MnbzOmVgQ12x"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SpeechRecognitionModel(n_cnn_layers, n_rnn_layers, rnn_dim,n_class, n_feats, stride, dropout).to(device)"
      ],
      "metadata": {
        "id": "CWjBGM4UQ12y"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def GreedyDecoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n",
        "    arg_maxes = torch.argmax(output, dim=2) # arg max n classes\n",
        "    decodes = []\n",
        "    targets = []\n",
        "    for i, args in enumerate(arg_maxes): # for sample in batch\n",
        "        decode = []\n",
        "        targets.append(index_to_label(labels[i][:label_lengths[i]].tolist()))\n",
        "        for j, index in enumerate(args): # for frame in sample\n",
        "            if index != blank_label:\n",
        "                if collapse_repeated and j != 0 and index == args[j -1]:\n",
        "                    continue\n",
        "                decode.append(index.item())\n",
        "        decodes.append(index_to_label(decode))\n",
        "    return decodes, targets"
      ],
      "metadata": {
        "id": "0SqhMj2-Q12y"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jiwer\n",
        "from jiwer import wer\n",
        "from jiwer import cer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3LGowzKQ12y",
        "outputId": "0fbcbb60-1e87-49da-b60c-2c8a1974bc4a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: jiwer in /usr/local/lib/python3.8/dist-packages (2.5.1)\n",
            "Requirement already satisfied: levenshtein==0.20.2 in /usr/local/lib/python3.8/dist-packages (from jiwer) (0.20.2)\n",
            "Requirement already satisfied: rapidfuzz<3.0.0,>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from levenshtein==0.20.2->jiwer) (2.13.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Results of Training\n",
        "clean_train_losses = []\n",
        "noisy_train_losses = []\n",
        "wers_epoch = []\n",
        "cers_epoch = []\n",
        "\n",
        "\n",
        "Training_Result = (clean_train_losses, noisy_train_losses, wers_epoch, cers_epoch)"
      ],
      "metadata": {
        "id": "zO2yV0QJQ12y"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "train_clean_loader = torch.utils.data.DataLoader(dataset=drvctk_train_dataset,\n",
        "                                batch_size=batch_size_train,\n",
        "                                shuffle=True,\n",
        "                                collate_fn=lambda x: processing_collate_fn(x, type1=\"train\", type2 = \"clean\"),\n",
        "                                **kwargs)\n",
        "test_clean_loader = torch.utils.data.DataLoader(dataset=drvctk_test_dataset,\n",
        "                            batch_size=batch_size_train,\n",
        "                            shuffle=False,\n",
        "                            collate_fn=lambda x: processing_collate_fn(x, type1=\"valid\", type2 = \"clean\"),\n",
        "                            **kwargs)\n",
        "\n",
        "train_noisy_loader = torch.utils.data.DataLoader(dataset=drvctk_train_dataset,\n",
        "                            batch_size=batch_size_train,\n",
        "                            shuffle=True,\n",
        "                            collate_fn=lambda x: processing_collate_fn(x, type1=\"train\", type2 = \"noisy\"),\n",
        "                            **kwargs)\n",
        "test_noisy_loader = torch.utils.data.DataLoader(dataset=drvctk_test_dataset,\n",
        "                            batch_size=batch_size_train,\n",
        "                            shuffle=False,\n",
        "                            collate_fn=lambda x: processing_collate_fn(x, type1=\"valid\", type2 = \"noisy\"),\n",
        "                            **kwargs)\n",
        "\n"
      ],
      "metadata": {
        "id": "zHUQySnoQ12z"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.AdamW(model.parameters(), learning_rate)\n",
        "criterion = nn.CTCLoss(blank=28, zero_infinity=True).to(device)\n",
        "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, \n",
        "                                        steps_per_epoch=int(len(train_clean_loader)),\n",
        "                                        epochs=20,\n",
        "                                        anneal_strategy='linear')"
      ],
      "metadata": {
        "id": "Y5pLHvYiQ12z"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math"
      ],
      "metadata": {
        "id": "ATtP_ZnvTNzQ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "# Train Function\n",
        "def train(Training_Result_p):\n",
        "\n",
        "  clean_train_losses, noisy_train_losses, wers_epoch, cers_epoch = Training_Result_p\n",
        "\n",
        "  # Train parameters\n",
        "  epoch = 1 # starting epoch\n",
        "\n",
        "  data_len = len(drvctk_train_dataset)\n",
        " \n",
        "  print(\"Training model\")\n",
        "\n",
        "  torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "  while ((epoch <= n_epochs) ):\n",
        "    print(\"Epoch :{}\".format(epoch))\n",
        "    print()\n",
        "    \n",
        "    # Training\n",
        "    model.train()\n",
        "    train_loss = [] # calc train loss\n",
        "    for batch_idx, _data in enumerate(train_clean_loader):\n",
        "\n",
        "      spectrograms, labels, input_lengths, label_lengths = _data \n",
        "\n",
        "      spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      output = model(spectrograms)  # (batch, time, n_class)\n",
        "      output = F.log_softmax(output, dim=2)\n",
        "      output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "\n",
        "      drop = False\n",
        "      for label_sequence in range(len(label_lengths)):\n",
        "          if label_lengths[label_sequence] > 512 or label_lengths[label_sequence]+1 >= input_lengths[label_sequence]:\n",
        "            drop = True\n",
        "\n",
        "      if (drop):\n",
        "        continue\n",
        "\n",
        "      loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "\n",
        "      \n",
        "      loss.backward()\n",
        "\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "      if (not math.isnan(loss.item())) and (loss.item() != float(\"inf\")):\n",
        "          train_loss.append(loss.item())\n",
        "      if batch_idx % 100 == 0 or batch_idx == data_len:\n",
        "          print('++++++++Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}++++++++++++++++'.format(\n",
        "              epoch, batch_idx * len(spectrograms), data_len,\n",
        "              100. * batch_idx / len(train_clean_loader), loss.item()))\n",
        "\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "    \n",
        "    clean_train_losses.append(train_loss / len(drvctk_train_dataset))\n",
        "\n",
        "    # Evaluation \n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    test_cer, test_wer = [], []\n",
        "    with torch.no_grad():\n",
        "      for batch_idx, _data in enumerate(test_clean_loader):\n",
        "\n",
        "        spectrograms, labels, input_lengths, label_lengths = _data \n",
        "\n",
        "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "        output = model(spectrograms)  # (batch, time, n_class)\n",
        "        output = F.log_softmax(output, dim=2)\n",
        "        output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "\n",
        "        loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "        test_loss += loss.item() \n",
        "\n",
        "        decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
        "\n",
        "        for j in range(len(decoded_preds)):\n",
        "            test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
        "            test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
        "\n",
        "\n",
        "    avg_cer = sum(test_cer)/len(test_cer)\n",
        "    avg_wer = sum(test_wer)/len(test_wer)\n",
        "\n",
        "    wers_epoch.append(avg_wer)\n",
        "    cers_epoch.append(avg_cer)\n",
        "\n",
        "    print('Epoch: {}, Test Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(epoch,test_loss/len(drvctk_test_dataset), avg_cer, avg_wer))\n",
        "\n",
        "    torch.save(model, '/content/drive/My Drive/DLS_Project/best_deep_s.pt')\n",
        "\n",
        "    print(\"Switching to Noisy Data\")\n",
        "\n",
        "    # Training\n",
        "    model.train()\n",
        "    train_loss = [] # calc train loss\n",
        "    for batch_idx, _data in enumerate(train_noisy_loader):\n",
        "\n",
        "      spectrograms, labels, input_lengths, label_lengths = _data \n",
        "\n",
        "      spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      output = model(spectrograms)  # (batch, time, n_class)\n",
        "      output = F.log_softmax(output, dim=2)\n",
        "      output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "\n",
        "      drop = False\n",
        "      for label_sequence in range(len(label_lengths)):\n",
        "          if label_lengths[label_sequence] > 512 or label_lengths[label_sequence]+1 >= input_lengths[label_sequence]:\n",
        "            drop = True\n",
        "\n",
        "      if (drop):\n",
        "        continue\n",
        "\n",
        "      loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "\n",
        "      \n",
        "      loss.backward()\n",
        "\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "      if (not math.isnan(loss.item())) and (loss.item() != float(\"inf\")):\n",
        "          train_loss.append(loss.item())\n",
        "      if batch_idx % 100 == 0 or batch_idx == data_len:\n",
        "          print('++++++++Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}++++++++++++++++'.format(\n",
        "              epoch, batch_idx * len(spectrograms), data_len,\n",
        "              100. * batch_idx / len(train_noisy_loader), loss.item()))\n",
        "\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "    \n",
        "    noisy_train_losses.append(train_loss / len(drvctk_train_dataset))\n",
        "\n",
        "    # Evaluation \n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    test_cer, test_wer = [], []\n",
        "    with torch.no_grad():\n",
        "      for batch_idx, _data in enumerate(test_noisy_loader):\n",
        "\n",
        "        spectrograms, labels, input_lengths, label_lengths = _data \n",
        "\n",
        "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "        output = model(spectrograms)  # (batch, time, n_class)\n",
        "        output = F.log_softmax(output, dim=2)\n",
        "        output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "\n",
        "        loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "        test_loss += loss.item() \n",
        "\n",
        "        decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
        "\n",
        "        for j in range(len(decoded_preds)):\n",
        "            test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
        "            test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
        "\n",
        "\n",
        "    avg_cer = sum(test_cer)/len(test_cer)\n",
        "    avg_wer = sum(test_wer)/len(test_wer)\n",
        "\n",
        "    wers_epoch.append(avg_wer)\n",
        "    cers_epoch.append(avg_cer)\n",
        "\n",
        "    print('Epoch: {}, Test Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(epoch,test_loss/len(drvctk_test_dataset), avg_cer, avg_wer))\n",
        "\n",
        "    torch.save(model, '/content/drive/My Drive/DLS_Project/best_deep_s.pt')\n",
        "    \n",
        "    # increment epoch after both clean and noisy data\n",
        "    epoch += 1\n",
        "  \n",
        "\n",
        "  Training_Return = (clean_train_losses, noisy_train_losses, wers_epoch, cers_epoch)\n",
        "  return Training_Return"
      ],
      "metadata": {
        "id": "GWpp7YKqQ12z"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train \n",
        "print(model)\n",
        "print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
        "\n",
        "print(\"Training!\")\n",
        "res = train(Training_Result)\n",
        "\n",
        "clean_train_losses, noisy_train_losses, wers_epoch, cers_epoch = res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MokpazodCvf5",
        "outputId": "7f5d31c1-4c83-4199-ca71-b8c7f54853e6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SpeechRecognitionModel(\n",
            "  (cnn): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (rescnn_layers): Sequential(\n",
            "    (0): ResidualCNN(\n",
            "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (dropout1): Dropout(p=0.1, inplace=False)\n",
            "      (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      (layer_norm1): CNNLayerNorm(\n",
            "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (layer_norm2): CNNLayerNorm(\n",
            "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (1): ResidualCNN(\n",
            "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (dropout1): Dropout(p=0.1, inplace=False)\n",
            "      (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      (layer_norm1): CNNLayerNorm(\n",
            "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (layer_norm2): CNNLayerNorm(\n",
            "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (2): ResidualCNN(\n",
            "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (dropout1): Dropout(p=0.1, inplace=False)\n",
            "      (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      (layer_norm1): CNNLayerNorm(\n",
            "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (layer_norm2): CNNLayerNorm(\n",
            "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (fully_connected): Linear(in_features=2048, out_features=512, bias=True)\n",
            "  (birnn_layers): Sequential(\n",
            "    (0): BidirectionalGRU(\n",
            "      (BiGRU): GRU(512, 512, batch_first=True, bidirectional=True)\n",
            "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (1): BidirectionalGRU(\n",
            "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
            "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (2): BidirectionalGRU(\n",
            "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
            "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (3): BidirectionalGRU(\n",
            "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
            "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (4): BidirectionalGRU(\n",
            "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
            "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    (1): GELU(approximate='none')\n",
            "    (2): Dropout(p=0.1, inplace=False)\n",
            "    (3): Linear(in_features=512, out_features=29, bias=True)\n",
            "  )\n",
            ")\n",
            "Num Model Parameters 23705373\n",
            "Training!\n",
            "Epoch :1\n",
            "\n",
            "++++++++Train Epoch: 1 [0/11572 (0%)]\tLoss: 9.111658++++++++++++++++\n",
            "++++++++Train Epoch: 1 [1500/11572 (13%)]\tLoss: 2.935668++++++++++++++++\n",
            "++++++++Train Epoch: 1 [3000/11572 (26%)]\tLoss: 2.948955++++++++++++++++\n",
            "++++++++Train Epoch: 1 [4500/11572 (39%)]\tLoss: 2.953876++++++++++++++++\n",
            "++++++++Train Epoch: 1 [6000/11572 (52%)]\tLoss: 2.974118++++++++++++++++\n",
            "++++++++Train Epoch: 1 [7500/11572 (65%)]\tLoss: 2.884999++++++++++++++++\n",
            "++++++++Train Epoch: 1 [9000/11572 (78%)]\tLoss: 2.836777++++++++++++++++\n",
            "++++++++Train Epoch: 1 [10500/11572 (91%)]\tLoss: 2.803380++++++++++++++++\n",
            "\n",
            "evaluating…\n",
            "Test set: Average loss: 2.6710, Average CER: 0.930509 Average WER: 0.9971\n",
            "\n",
            "Switching to Noisy Data\n",
            "++++++++Train Epoch: 1 [0/11572 (0%)]\tLoss: 2.980884++++++++++++++++\n",
            "++++++++Train Epoch: 1 [1500/11572 (13%)]\tLoss: 2.644198++++++++++++++++\n",
            "++++++++Train Epoch: 1 [3000/11572 (26%)]\tLoss: 2.549384++++++++++++++++\n",
            "++++++++Train Epoch: 1 [4500/11572 (39%)]\tLoss: 2.505565++++++++++++++++\n",
            "++++++++Train Epoch: 1 [6000/11572 (52%)]\tLoss: 2.540947++++++++++++++++\n",
            "++++++++Train Epoch: 1 [7500/11572 (65%)]\tLoss: 2.453830++++++++++++++++\n",
            "++++++++Train Epoch: 1 [9000/11572 (78%)]\tLoss: 2.431526++++++++++++++++\n",
            "++++++++Train Epoch: 1 [10500/11572 (91%)]\tLoss: 2.410410++++++++++++++++\n",
            "\n",
            "evaluating…\n",
            "Test set: Average loss: 2.4254, Average CER: 0.677178 Average WER: 1.0924\n",
            "\n",
            "Epoch :2\n",
            "\n",
            "++++++++Train Epoch: 2 [0/11572 (0%)]\tLoss: 2.317916++++++++++++++++\n",
            "++++++++Train Epoch: 2 [1500/11572 (13%)]\tLoss: 2.101047++++++++++++++++\n",
            "++++++++Train Epoch: 2 [3000/11572 (26%)]\tLoss: 2.040599++++++++++++++++\n",
            "++++++++Train Epoch: 2 [4500/11572 (39%)]\tLoss: 2.172238++++++++++++++++\n",
            "++++++++Train Epoch: 2 [6000/11572 (52%)]\tLoss: 2.058380++++++++++++++++\n",
            "++++++++Train Epoch: 2 [7500/11572 (65%)]\tLoss: 1.948295++++++++++++++++\n",
            "++++++++Train Epoch: 2 [9000/11572 (78%)]\tLoss: 1.724047++++++++++++++++\n",
            "++++++++Train Epoch: 2 [10500/11572 (91%)]\tLoss: 1.711436++++++++++++++++\n",
            "\n",
            "evaluating…\n",
            "Test set: Average loss: 1.8517, Average CER: 0.509671 Average WER: 0.9533\n",
            "\n",
            "Switching to Noisy Data\n",
            "++++++++Train Epoch: 2 [0/11572 (0%)]\tLoss: 2.588332++++++++++++++++\n",
            "++++++++Train Epoch: 2 [1500/11572 (13%)]\tLoss: 2.214920++++++++++++++++\n",
            "++++++++Train Epoch: 2 [3000/11572 (26%)]\tLoss: 2.145808++++++++++++++++\n",
            "++++++++Train Epoch: 2 [4500/11572 (39%)]\tLoss: 2.125967++++++++++++++++\n",
            "++++++++Train Epoch: 2 [6000/11572 (52%)]\tLoss: 2.047584++++++++++++++++\n",
            "++++++++Train Epoch: 2 [7500/11572 (65%)]\tLoss: 1.919502++++++++++++++++\n",
            "++++++++Train Epoch: 2 [9000/11572 (78%)]\tLoss: 2.109195++++++++++++++++\n",
            "++++++++Train Epoch: 2 [10500/11572 (91%)]\tLoss: 2.000777++++++++++++++++\n",
            "\n",
            "evaluating…\n",
            "Test set: Average loss: 2.1152, Average CER: 0.573457 Average WER: 0.9402\n",
            "\n",
            "Epoch :3\n",
            "\n",
            "++++++++Train Epoch: 3 [0/11572 (0%)]\tLoss: 1.956741++++++++++++++++\n",
            "++++++++Train Epoch: 3 [1500/11572 (13%)]\tLoss: 1.766502++++++++++++++++\n",
            "++++++++Train Epoch: 3 [3000/11572 (26%)]\tLoss: 1.769219++++++++++++++++\n",
            "++++++++Train Epoch: 3 [4500/11572 (39%)]\tLoss: 1.662487++++++++++++++++\n",
            "++++++++Train Epoch: 3 [6000/11572 (52%)]\tLoss: 1.549981++++++++++++++++\n",
            "++++++++Train Epoch: 3 [7500/11572 (65%)]\tLoss: 1.610122++++++++++++++++\n",
            "++++++++Train Epoch: 3 [9000/11572 (78%)]\tLoss: 1.521779++++++++++++++++\n",
            "++++++++Train Epoch: 3 [10500/11572 (91%)]\tLoss: 1.531994++++++++++++++++\n",
            "\n",
            "evaluating…\n",
            "Test set: Average loss: 1.6594, Average CER: 0.463110 Average WER: 0.8575\n",
            "\n",
            "Switching to Noisy Data\n",
            "++++++++Train Epoch: 3 [0/11572 (0%)]\tLoss: 2.230558++++++++++++++++\n",
            "++++++++Train Epoch: 3 [1500/11572 (13%)]\tLoss: 1.827030++++++++++++++++\n",
            "++++++++Train Epoch: 3 [3000/11572 (26%)]\tLoss: 1.851451++++++++++++++++\n",
            "++++++++Train Epoch: 3 [4500/11572 (39%)]\tLoss: 1.982674++++++++++++++++\n",
            "++++++++Train Epoch: 3 [6000/11572 (52%)]\tLoss: 1.781752++++++++++++++++\n",
            "++++++++Train Epoch: 3 [7500/11572 (65%)]\tLoss: 1.704154++++++++++++++++\n",
            "++++++++Train Epoch: 3 [9000/11572 (78%)]\tLoss: 1.634221++++++++++++++++\n",
            "++++++++Train Epoch: 3 [10500/11572 (91%)]\tLoss: 1.901335++++++++++++++++\n",
            "\n",
            "evaluating…\n",
            "Test set: Average loss: 2.0128, Average CER: 0.546908 Average WER: 0.9092\n",
            "\n",
            "Epoch :4\n",
            "\n",
            "++++++++Train Epoch: 4 [0/11572 (0%)]\tLoss: 1.796937++++++++++++++++\n",
            "++++++++Train Epoch: 4 [1500/11572 (13%)]\tLoss: 1.666717++++++++++++++++\n",
            "++++++++Train Epoch: 4 [3000/11572 (26%)]\tLoss: 1.444167++++++++++++++++\n",
            "++++++++Train Epoch: 4 [4500/11572 (39%)]\tLoss: 1.689073++++++++++++++++\n",
            "++++++++Train Epoch: 4 [6000/11572 (52%)]\tLoss: 1.526284++++++++++++++++\n",
            "++++++++Train Epoch: 4 [7500/11572 (65%)]\tLoss: 1.311661++++++++++++++++\n",
            "++++++++Train Epoch: 4 [9000/11572 (78%)]\tLoss: 1.464658++++++++++++++++\n",
            "++++++++Train Epoch: 4 [10500/11572 (91%)]\tLoss: 1.519082++++++++++++++++\n",
            "\n",
            "evaluating…\n",
            "Test set: Average loss: 1.4143, Average CER: 0.391401 Average WER: 0.8079\n",
            "\n",
            "Switching to Noisy Data\n",
            "++++++++Train Epoch: 4 [0/11572 (0%)]\tLoss: 1.939929++++++++++++++++\n",
            "++++++++Train Epoch: 4 [1500/11572 (13%)]\tLoss: 1.685738++++++++++++++++\n",
            "++++++++Train Epoch: 4 [3000/11572 (26%)]\tLoss: 1.671604++++++++++++++++\n",
            "++++++++Train Epoch: 4 [4500/11572 (39%)]\tLoss: 1.624133++++++++++++++++\n",
            "++++++++Train Epoch: 4 [6000/11572 (52%)]\tLoss: 1.700594++++++++++++++++\n",
            "++++++++Train Epoch: 4 [7500/11572 (65%)]\tLoss: 1.568037++++++++++++++++\n",
            "++++++++Train Epoch: 4 [9000/11572 (78%)]\tLoss: 1.638615++++++++++++++++\n",
            "++++++++Train Epoch: 4 [10500/11572 (91%)]\tLoss: 1.633936++++++++++++++++\n",
            "\n",
            "evaluating…\n",
            "Test set: Average loss: 1.8617, Average CER: 0.507901 Average WER: 0.8830\n",
            "\n",
            "Epoch :5\n",
            "\n",
            "++++++++Train Epoch: 5 [0/11572 (0%)]\tLoss: 1.599609++++++++++++++++\n",
            "++++++++Train Epoch: 5 [1500/11572 (13%)]\tLoss: 1.458287++++++++++++++++\n",
            "++++++++Train Epoch: 5 [3000/11572 (26%)]\tLoss: 1.226761++++++++++++++++\n",
            "++++++++Train Epoch: 5 [4500/11572 (39%)]\tLoss: 1.224484++++++++++++++++\n",
            "++++++++Train Epoch: 5 [6000/11572 (52%)]\tLoss: 1.342900++++++++++++++++\n",
            "++++++++Train Epoch: 5 [7500/11572 (65%)]\tLoss: 1.232685++++++++++++++++\n",
            "++++++++Train Epoch: 5 [9000/11572 (78%)]\tLoss: 1.383735++++++++++++++++\n",
            "++++++++Train Epoch: 5 [10500/11572 (91%)]\tLoss: 1.558535++++++++++++++++\n",
            "\n",
            "evaluating…\n",
            "Test set: Average loss: 1.3443, Average CER: 0.369921 Average WER: 0.7961\n",
            "\n",
            "Switching to Noisy Data\n",
            "++++++++Train Epoch: 5 [0/11572 (0%)]\tLoss: 1.866364++++++++++++++++\n",
            "++++++++Train Epoch: 5 [1500/11572 (13%)]\tLoss: 1.537132++++++++++++++++\n",
            "++++++++Train Epoch: 5 [3000/11572 (26%)]\tLoss: 1.656196++++++++++++++++\n",
            "++++++++Train Epoch: 5 [4500/11572 (39%)]\tLoss: 1.581265++++++++++++++++\n",
            "++++++++Train Epoch: 5 [6000/11572 (52%)]\tLoss: 1.777394++++++++++++++++\n",
            "++++++++Train Epoch: 5 [7500/11572 (65%)]\tLoss: 1.559584++++++++++++++++\n",
            "++++++++Train Epoch: 5 [9000/11572 (78%)]\tLoss: 1.719077++++++++++++++++\n",
            "++++++++Train Epoch: 5 [10500/11572 (91%)]\tLoss: 1.338716++++++++++++++++\n",
            "\n",
            "evaluating…\n",
            "Test set: Average loss: 1.8379, Average CER: 0.489388 Average WER: 0.8648\n",
            "\n",
            "Epoch :6\n",
            "\n",
            "++++++++Train Epoch: 6 [0/11572 (0%)]\tLoss: 1.392210++++++++++++++++\n",
            "++++++++Train Epoch: 6 [1500/11572 (13%)]\tLoss: 1.102595++++++++++++++++\n",
            "++++++++Train Epoch: 6 [3000/11572 (26%)]\tLoss: 1.083212++++++++++++++++\n",
            "++++++++Train Epoch: 6 [4500/11572 (39%)]\tLoss: 1.041366++++++++++++++++\n",
            "++++++++Train Epoch: 6 [6000/11572 (52%)]\tLoss: 1.237802++++++++++++++++\n",
            "++++++++Train Epoch: 6 [7500/11572 (65%)]\tLoss: 1.641379++++++++++++++++\n",
            "++++++++Train Epoch: 6 [9000/11572 (78%)]\tLoss: 0.981881++++++++++++++++\n",
            "++++++++Train Epoch: 6 [10500/11572 (91%)]\tLoss: 1.319110++++++++++++++++\n",
            "\n",
            "evaluating…\n",
            "Test set: Average loss: 1.2212, Average CER: 0.336914 Average WER: 0.7347\n",
            "\n",
            "Switching to Noisy Data\n",
            "++++++++Train Epoch: 6 [0/11572 (0%)]\tLoss: 2.097785++++++++++++++++\n",
            "++++++++Train Epoch: 6 [1500/11572 (13%)]\tLoss: 1.393673++++++++++++++++\n",
            "++++++++Train Epoch: 6 [3000/11572 (26%)]\tLoss: 1.301658++++++++++++++++\n",
            "++++++++Train Epoch: 6 [4500/11572 (39%)]\tLoss: 1.375435++++++++++++++++\n",
            "++++++++Train Epoch: 6 [6000/11572 (52%)]\tLoss: 1.603002++++++++++++++++\n",
            "++++++++Train Epoch: 6 [7500/11572 (65%)]\tLoss: 1.338153++++++++++++++++\n",
            "++++++++Train Epoch: 6 [9000/11572 (78%)]\tLoss: 1.570657++++++++++++++++\n",
            "++++++++Train Epoch: 6 [10500/11572 (91%)]\tLoss: 1.161834++++++++++++++++\n",
            "\n",
            "evaluating…\n",
            "Test set: Average loss: 1.7787, Average CER: 0.458527 Average WER: 0.8468\n",
            "\n",
            "Epoch :7\n",
            "\n",
            "++++++++Train Epoch: 7 [0/11572 (0%)]\tLoss: 1.115704++++++++++++++++\n",
            "++++++++Train Epoch: 7 [1500/11572 (13%)]\tLoss: 0.847106++++++++++++++++\n",
            "++++++++Train Epoch: 7 [3000/11572 (26%)]\tLoss: 1.150842++++++++++++++++\n",
            "++++++++Train Epoch: 7 [4500/11572 (39%)]\tLoss: 1.290024++++++++++++++++\n",
            "++++++++Train Epoch: 7 [6000/11572 (52%)]\tLoss: 1.000737++++++++++++++++\n",
            "++++++++Train Epoch: 7 [7500/11572 (65%)]\tLoss: 1.256523++++++++++++++++\n",
            "++++++++Train Epoch: 7 [9000/11572 (78%)]\tLoss: 1.051111++++++++++++++++\n",
            "++++++++Train Epoch: 7 [10500/11572 (91%)]\tLoss: 0.871871++++++++++++++++\n",
            "\n",
            "evaluating…\n",
            "Test set: Average loss: 1.1649, Average CER: 0.313305 Average WER: 0.7080\n",
            "\n",
            "Switching to Noisy Data\n",
            "++++++++Train Epoch: 7 [0/11572 (0%)]\tLoss: 1.684759++++++++++++++++\n",
            "++++++++Train Epoch: 7 [1500/11572 (13%)]\tLoss: 1.321365++++++++++++++++\n",
            "++++++++Train Epoch: 7 [3000/11572 (26%)]\tLoss: 1.830627++++++++++++++++\n",
            "++++++++Train Epoch: 7 [4500/11572 (39%)]\tLoss: 1.207624++++++++++++++++\n",
            "++++++++Train Epoch: 7 [6000/11572 (52%)]\tLoss: 1.235748++++++++++++++++\n",
            "++++++++Train Epoch: 7 [7500/11572 (65%)]\tLoss: 0.977576++++++++++++++++\n",
            "++++++++Train Epoch: 7 [9000/11572 (78%)]\tLoss: 1.215955++++++++++++++++\n",
            "++++++++Train Epoch: 7 [10500/11572 (91%)]\tLoss: 1.172051++++++++++++++++\n",
            "\n",
            "evaluating…\n",
            "Test set: Average loss: 1.7463, Average CER: 0.456870 Average WER: 0.8355\n",
            "\n",
            "Epoch :8\n",
            "\n",
            "++++++++Train Epoch: 8 [0/11572 (0%)]\tLoss: 1.127429++++++++++++++++\n",
            "++++++++Train Epoch: 8 [1500/11572 (13%)]\tLoss: 0.996734++++++++++++++++\n",
            "++++++++Train Epoch: 8 [3000/11572 (26%)]\tLoss: 1.062656++++++++++++++++\n",
            "++++++++Train Epoch: 8 [4500/11572 (39%)]\tLoss: 0.982256++++++++++++++++\n",
            "++++++++Train Epoch: 8 [6000/11572 (52%)]\tLoss: 0.854808++++++++++++++++\n",
            "++++++++Train Epoch: 8 [7500/11572 (65%)]\tLoss: 0.961772++++++++++++++++\n",
            "++++++++Train Epoch: 8 [9000/11572 (78%)]\tLoss: 0.868561++++++++++++++++\n",
            "++++++++Train Epoch: 8 [10500/11572 (91%)]\tLoss: 0.992252++++++++++++++++\n",
            "\n",
            "evaluating…\n",
            "Test set: Average loss: 1.1201, Average CER: 0.297962 Average WER: 0.6927\n",
            "\n",
            "Switching to Noisy Data\n",
            "++++++++Train Epoch: 8 [0/11572 (0%)]\tLoss: 1.693237++++++++++++++++\n",
            "++++++++Train Epoch: 8 [1500/11572 (13%)]\tLoss: 1.260140++++++++++++++++\n",
            "++++++++Train Epoch: 8 [3000/11572 (26%)]\tLoss: 1.229536++++++++++++++++\n",
            "++++++++Train Epoch: 8 [4500/11572 (39%)]\tLoss: 1.188975++++++++++++++++\n",
            "++++++++Train Epoch: 8 [6000/11572 (52%)]\tLoss: 0.998223++++++++++++++++\n",
            "++++++++Train Epoch: 8 [7500/11572 (65%)]\tLoss: 1.169791++++++++++++++++\n",
            "++++++++Train Epoch: 8 [9000/11572 (78%)]\tLoss: 1.348370++++++++++++++++\n",
            "++++++++Train Epoch: 8 [10500/11572 (91%)]\tLoss: 1.044956++++++++++++++++\n",
            "\n",
            "evaluating…\n",
            "Test set: Average loss: 1.7156, Average CER: 0.447331 Average WER: 0.8325\n",
            "\n",
            "Epoch :9\n",
            "\n",
            "++++++++Train Epoch: 9 [0/11572 (0%)]\tLoss: 1.064769++++++++++++++++\n",
            "++++++++Train Epoch: 9 [1500/11572 (13%)]\tLoss: 0.953775++++++++++++++++\n",
            "++++++++Train Epoch: 9 [3000/11572 (26%)]\tLoss: 0.885044++++++++++++++++\n",
            "++++++++Train Epoch: 9 [4500/11572 (39%)]\tLoss: 0.806414++++++++++++++++\n",
            "++++++++Train Epoch: 9 [6000/11572 (52%)]\tLoss: 0.766269++++++++++++++++\n",
            "++++++++Train Epoch: 9 [7500/11572 (65%)]\tLoss: 1.029967++++++++++++++++\n",
            "++++++++Train Epoch: 9 [9000/11572 (78%)]\tLoss: 0.866122++++++++++++++++\n",
            "++++++++Train Epoch: 9 [10500/11572 (91%)]\tLoss: 0.721280++++++++++++++++\n",
            "\n",
            "evaluating…\n",
            "Test set: Average loss: 1.1025, Average CER: 0.288154 Average WER: 0.6760\n",
            "\n",
            "Switching to Noisy Data\n",
            "++++++++Train Epoch: 9 [0/11572 (0%)]\tLoss: 1.423221++++++++++++++++\n",
            "++++++++Train Epoch: 9 [1500/11572 (13%)]\tLoss: 1.214900++++++++++++++++\n",
            "++++++++Train Epoch: 9 [3000/11572 (26%)]\tLoss: 1.210425++++++++++++++++\n",
            "++++++++Train Epoch: 9 [4500/11572 (39%)]\tLoss: 0.843606++++++++++++++++\n",
            "++++++++Train Epoch: 9 [6000/11572 (52%)]\tLoss: 1.050982++++++++++++++++\n",
            "++++++++Train Epoch: 9 [7500/11572 (65%)]\tLoss: 0.860864++++++++++++++++\n",
            "++++++++Train Epoch: 9 [9000/11572 (78%)]\tLoss: 1.087700++++++++++++++++\n",
            "++++++++Train Epoch: 9 [10500/11572 (91%)]\tLoss: 1.345636++++++++++++++++\n",
            "\n",
            "evaluating…\n",
            "Test set: Average loss: 1.7616, Average CER: 0.437237 Average WER: 0.8181\n",
            "\n",
            "Epoch :10\n",
            "\n",
            "++++++++Train Epoch: 10 [0/11572 (0%)]\tLoss: 1.228477++++++++++++++++\n",
            "++++++++Train Epoch: 10 [1500/11572 (13%)]\tLoss: 0.823864++++++++++++++++\n",
            "++++++++Train Epoch: 10 [3000/11572 (26%)]\tLoss: 0.784857++++++++++++++++\n",
            "++++++++Train Epoch: 10 [4500/11572 (39%)]\tLoss: 0.677880++++++++++++++++\n",
            "++++++++Train Epoch: 10 [6000/11572 (52%)]\tLoss: 0.674563++++++++++++++++\n",
            "++++++++Train Epoch: 10 [7500/11572 (65%)]\tLoss: 0.722994++++++++++++++++\n",
            "++++++++Train Epoch: 10 [9000/11572 (78%)]\tLoss: 0.757970++++++++++++++++\n",
            "++++++++Train Epoch: 10 [10500/11572 (91%)]\tLoss: 0.737419++++++++++++++++\n",
            "\n",
            "evaluating…\n",
            "Test set: Average loss: 1.0822, Average CER: 0.277984 Average WER: 0.6559\n",
            "\n",
            "Switching to Noisy Data\n",
            "++++++++Train Epoch: 10 [0/11572 (0%)]\tLoss: 1.201898++++++++++++++++\n",
            "++++++++Train Epoch: 10 [1500/11572 (13%)]\tLoss: 1.131637++++++++++++++++\n",
            "++++++++Train Epoch: 10 [3000/11572 (26%)]\tLoss: 1.015085++++++++++++++++\n",
            "++++++++Train Epoch: 10 [4500/11572 (39%)]\tLoss: 0.766638++++++++++++++++\n",
            "++++++++Train Epoch: 10 [6000/11572 (52%)]\tLoss: 0.889277++++++++++++++++\n",
            "++++++++Train Epoch: 10 [7500/11572 (65%)]\tLoss: 0.900597++++++++++++++++\n",
            "++++++++Train Epoch: 10 [9000/11572 (78%)]\tLoss: 1.001585++++++++++++++++\n",
            "++++++++Train Epoch: 10 [10500/11572 (91%)]\tLoss: 0.996027++++++++++++++++\n",
            "\n",
            "evaluating…\n",
            "Test set: Average loss: 1.7430, Average CER: 0.435004 Average WER: 0.8135\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.linspace(1, 10, 10)\n",
        "plt.plot(x, clean_train_losses)\n",
        "plt.xlabel('Epochs')\n",
        "plt.title(\"CNN-RNN Training on DR-VCTK Clean Loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rNsxHmOOCvkh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "777e0060-4a15-49cb-ccd3-ae1f497b83e2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.linspace(1, 10, 10)\n",
        "plt.plot(x, noisy_train_losses)\n",
        "plt.xlabel('Epochs')\n",
        "plt.title(\"CNN-RNN Training on DR-VCTK Noisy Loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "v89VgJrpCvll",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "e2b435c4-0684-467f-fd81-ee9994ce6344"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TsW1o-UECvpp"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rZJoxCSACvrJ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GXhwQnXRCvvC"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7JaSi71ECvwf"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lGPUBqXuCv06"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sCa-onO6Cv2Z"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_EXq4Uj14kir"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M1VORrTD4kmq"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ePcSXl-44kpZ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pOBIIUQv4ksS"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UxNMpRlX4kvB"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Old attempt wasnt working well"
      ],
      "metadata": {
        "id": "xDxKQf_Z4kyf"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# labels = [ \"'\",'|', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
        "# print(len(labels))"
      ],
      "metadata": {
        "id": "GKtwJOx9l6tC"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode trasncripts\n",
        "\n",
        "# def label_to_index(word):\n",
        "#     res = []\n",
        "#     for i in range(len(word)):\n",
        "#       res.append(labels.index(word[i]))\n",
        "#     # Return the position of the word in labels\n",
        "#     return res\n",
        "\n",
        "# def index_to_label(index):\n",
        "#     # Return the word corresponding to the index in labels\n",
        "#     # This is the inverse of label_to_index\n",
        "#     res = []\n",
        "#     for i in index:\n",
        "#       ch = labels[i]\n",
        "#       res.append(ch)\n",
        "#     return \"\".join(res)"
      ],
      "metadata": {
        "id": "Cqw7DpU7l6wB"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Prepreoccessing the spectrograms for train and validation\n",
        "# train_audio_transforms = torch.nn.Sequential(\n",
        "#     torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n",
        "#     torchaudio.transforms.FrequencyMasking(freq_mask_param=15),\n",
        "#     torchaudio.transforms.TimeMasking(time_mask_param=35)\n",
        "# )\n",
        "\n",
        "# valid_audio_transforms = torchaudio.transforms.MelSpectrogram()"
      ],
      "metadata": {
        "id": "TAJEG-3n5FmO"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# def processing(data, type_data=\"train\", type_data2 = \"clean\"):\n",
        "\n",
        "#     spectrograms = []\n",
        "#     labels = []\n",
        "#     input_lengths = []\n",
        "#     label_lengths = []\n",
        "\n",
        "#     for clean_waveform, _, noisy_waveform, _, speaker_id, utterance_id, _, _ in data:\n",
        "\n",
        "#         if type_data == 'train':\n",
        "#           if type_data2 == \"clean\":\n",
        "#             spec = train_audio_transforms(clean_waveform).squeeze(0).transpose(0, 1)\n",
        "#           else:\n",
        "#             spec = train_audio_transforms(noisy_waveform).squeeze(0).transpose(0, 1)\n",
        "#         else:\n",
        "#             if type_data2 == \"clean\":\n",
        "#               spec = valid_audio_transforms(clean_waveform).squeeze(0).transpose(0, 1)\n",
        "#             else:\n",
        "#               spec = valid_audio_transforms(noisy_waveform).squeeze(0).transpose(0, 1)\n",
        "\n",
        "\n",
        "#         spectrograms.append(spec)\n",
        "#         label = torch.Tensor(label_to_index(speaker_utterance_final[speaker_id][utterance_id]))\n",
        "#         labels.append(label)\n",
        "#         input_lengths.append(spec.shape[0]//2)\n",
        "#         label_lengths.append(len(label))\n",
        "\n",
        "#     spectrograms = torch.nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
        "#     labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
        "\n",
        "#     return spectrograms, labels, input_lengths, label_lengths"
      ],
      "metadata": {
        "id": "bwpwXeb93rvu"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2z4RAjS3y4K7"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Building the network\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# import torch.optim as optim\n",
        "\n",
        "# class CNNLayerNorm(nn.Module):\n",
        "#     # \"\"\"Layer normalization built for cnns input\"\"\"\n",
        "#     def __init__(self, n_feats):\n",
        "#         super(CNNLayerNorm, self).__init__()\n",
        "#         self.layer_norm = nn.LayerNorm(n_feats)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # x (batch, channel, feature, time)\n",
        "#         x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n",
        "#         x = self.layer_norm(x)\n",
        "#         return x.transpose(2, 3).contiguous() # (batch, channel, feature, time) \n",
        "\n",
        "# class ResidualCNN(nn.Module):\n",
        "#     # \"\"\"Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf\n",
        "#     #     except with layer norm instead of batch norm\n",
        "#     # \"\"\"\n",
        "#     def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
        "#         super(ResidualCNN, self).__init__()\n",
        "\n",
        "#         self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n",
        "#         self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n",
        "#         self.dropout1 = nn.Dropout(dropout)\n",
        "#         self.dropout2 = nn.Dropout(dropout)\n",
        "#         self.layer_norm1 = CNNLayerNorm(n_feats)\n",
        "#         self.layer_norm2 = CNNLayerNorm(n_feats)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         residual = x  # (batch, channel, feature, time)\n",
        "#         x = self.layer_norm1(x)\n",
        "#         x = F.gelu(x)\n",
        "#         x = self.dropout1(x)\n",
        "#         x = self.cnn1(x)\n",
        "#         x = self.layer_norm2(x)\n",
        "#         x = F.gelu(x)\n",
        "#         x = self.dropout2(x)\n",
        "#         x = self.cnn2(x)\n",
        "#         x += residual\n",
        "#         return x # (batch, channel, feature, time)\n",
        "        \n",
        "# class BidirectionalGRU(nn.Module):\n",
        "\n",
        "#     def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
        "#         super(BidirectionalGRU, self).__init__()\n",
        "\n",
        "#         self.BiGRU = nn.GRU(\n",
        "#             input_size=rnn_dim, hidden_size=hidden_size,\n",
        "#             num_layers=1, batch_first=batch_first, bidirectional=True)\n",
        "#         self.layer_norm = nn.LayerNorm(rnn_dim)\n",
        "#         self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.layer_norm(x)\n",
        "#         x = F.gelu(x)\n",
        "#         x, _ = self.BiGRU(x)\n",
        "#         x = self.dropout(x)\n",
        "#         return x\n",
        "\n",
        "\n",
        "# class SpeechRecognitionModel(nn.Module):\n",
        "#     # \"\"\"Speech Recognition Model Inspired by DeepSpeech 2\"\"\"\n",
        "\n",
        "#     def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):\n",
        "#         super(SpeechRecognitionModel, self).__init__()\n",
        "#         n_feats = n_feats//2\n",
        "#         self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3//2)  # cnn for extracting heirachal features\n",
        "\n",
        "#         # n residual cnn layers with filter size of 32\n",
        "#         self.rescnn_layers = nn.Sequential(*[\n",
        "#             ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats) \n",
        "#             for _ in range(n_cnn_layers)\n",
        "#         ])\n",
        "#         self.fully_connected = nn.Linear(n_feats*32, rnn_dim)\n",
        "#         self.birnn_layers = nn.Sequential(*[\n",
        "#             BidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n",
        "#                              hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n",
        "#             for i in range(n_rnn_layers)\n",
        "#         ])\n",
        "#         self.classifier = nn.Sequential(\n",
        "#             nn.Linear(rnn_dim*2, rnn_dim),  # birnn returns rnn_dim*2\n",
        "#             nn.GELU(),\n",
        "#             nn.Dropout(dropout),\n",
        "#             nn.Linear(rnn_dim, n_class)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.cnn(x)\n",
        "#         x = self.rescnn_layers(x)\n",
        "#         sizes = x.size()\n",
        "#         x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
        "#         x = x.transpose(1, 2) # (batch, time, feature)\n",
        "#         x = self.fully_connected(x)\n",
        "#         x = self.birnn_layers(x)\n",
        "#         x = self.classifier(x)\n",
        "#         return x"
      ],
      "metadata": {
        "id": "LNjqo1fn3ryh"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Hyperparameters\n",
        "# n_epochs = 10 # max_epochs\n",
        "# batch_size_train = 20 # batch size train set\n",
        "# batch_size_test = 20 # batch size test set\n",
        "# learning_rate = 5e-4 # learn rate\n",
        "# log_interval = 1 # log every _ epoch\n",
        "\n",
        "\n",
        "# n_cnn_layers = 3\n",
        "# n_rnn_layers = 3\n",
        "# rnn_dim = 512\n",
        "# n_class = 29\n",
        "# n_feats = 128\n",
        "# stride = 2\n",
        "# dropout = .1"
      ],
      "metadata": {
        "id": "NS0rfhWKy6Bv"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = SpeechRecognitionModel(n_cnn_layers, n_rnn_layers, rnn_dim,n_class, n_feats, stride, dropout).to(device)\n",
        "# print(model)\n",
        "# print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))"
      ],
      "metadata": {
        "id": "OzhxWrAqy0Jk"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def GreedyDecoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n",
        "#     arg_maxes = torch.argmax(output, dim=2) # arg max n classes\n",
        "#     decodes = []\n",
        "#     targets = []\n",
        "#     for i, args in enumerate(arg_maxes): # for sample in batch\n",
        "#         decode = []\n",
        "#         targets.append(index_to_label(labels[i][:label_lengths[i]].tolist()))\n",
        "#         for j, index in enumerate(args): # for frame in sample\n",
        "#             if index != blank_label:\n",
        "#                 if collapse_repeated and j != 0 and index == args[j -1]:\n",
        "#                     continue\n",
        "#                 decode.append(index.item())\n",
        "#         decodes.append(index_to_label(decode))\n",
        "#     return decodes, targets"
      ],
      "metadata": {
        "id": "OlnaigBx32M5"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install jiwer\n",
        "# from jiwer import wer\n",
        "# from jiwer import cer"
      ],
      "metadata": {
        "id": "K8EnMmC86tp1"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "\n",
        "# # Results of Training\n",
        "# losses = []\n",
        "# test_losses = []\n",
        "# wers_epoch = []\n",
        "# cers_epoch = []\n",
        "\n",
        "\n",
        "# Training_Result = (losses, test_losses, wers_epoch, cers_epoch)"
      ],
      "metadata": {
        "id": "90Pfju6GwE0Q"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use_cuda = torch.cuda.is_available()\n",
        "# kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "# clean_train_loader = torch.utils.data.DataLoader(dataset=drvctk_train_dataset,\n",
        "#                             batch_size=batch_size_train,\n",
        "#                             shuffle=True,\n",
        "#                             collate_fn=lambda x: processing(x, 'train',\"clean\"))\n",
        "# clean_test_loader = torch.utils.data.DataLoader(dataset=drvctk_test_dataset,\n",
        "#                             batch_size=batch_size_test,\n",
        "#                             shuffle=False,\n",
        "#                             collate_fn=lambda x: processing(x, 'valid', \"clean\"))"
      ],
      "metadata": {
        "id": "0DkQwe-2yQFR"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer = optim.AdamW(model.parameters(), learning_rate)\n",
        "# criterion = nn.CTCLoss(blank=28).to(device)\n",
        "# scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, \n",
        "#                                         steps_per_epoch=int(len(clean_train_loader)),\n",
        "#                                         epochs=n_epochs,\n",
        "#                                         anneal_strategy='linear')"
      ],
      "metadata": {
        "id": "UQmDT1Cozi--"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import copy\n",
        "\n",
        "# # Train Function\n",
        "# def train(Training_Result_p):\n",
        "\n",
        "#   losses, test_losses, wers_epoch, cers_epoch = Training_Result_p\n",
        "\n",
        "#   # Train parameters\n",
        "#   epoch = 1 # starting epoch\n",
        " \n",
        "\n",
        "#   print(\"Training model\")\n",
        "\n",
        "#   while ((epoch <= n_epochs) ):\n",
        "    \n",
        "#     # Training\n",
        "#     model.train()\n",
        "#     train_loss = 0 # calc train loss\n",
        "#     for batch_idx, _data in enumerate(clean_train_loader):\n",
        "\n",
        "#       spectrograms, labels, input_lengths, label_lengths = _data \n",
        "\n",
        "#       spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "#       optimizer.zero_grad()\n",
        "\n",
        "#       output = model(spectrograms)  # (batch, time, n_class)\n",
        "#       output = F.log_softmax(output, dim=2)\n",
        "#       output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "\n",
        "#       loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "#       loss.backward()\n",
        "\n",
        "#       train_loss += loss.item()\n",
        "\n",
        "#       if batch_idx % 100 == 0:\n",
        "#         print(\"Epoch: {} Batch: {}/ {}  Batch Loss: {}\".format(epoch, batch_idx, len(clean_train_loader), loss.item()))\n",
        "\n",
        "#       optimizer.step()\n",
        "#       scheduler.step()\n",
        "    \n",
        "#     losses.append(train_loss / len(drvctk_train_dataset))\n",
        "\n",
        "#     print('Epoch: {}, Train Average loss: {:.4f}\\n'.format(epoch,train_loss / len(drvctk_train_dataset)))\n",
        "\n",
        "#     # Evaluation \n",
        "#     model.eval()\n",
        "#     test_loss = 0\n",
        "#     test_cer, test_wer = [], []\n",
        "#     with torch.no_grad():\n",
        "#       for batch_idx, _data in enumerate(clean_test_loader):\n",
        "\n",
        "#         spectrograms, labels, input_lengths, label_lengths = _data \n",
        "\n",
        "#         spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "#         output = model(spectrograms)  # (batch, time, n_class)\n",
        "#         output = F.log_softmax(output, dim=2)\n",
        "#         output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "\n",
        "#         loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "#         test_loss += loss.item() \n",
        "\n",
        "#         decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
        "\n",
        "#         for j in range(len(decoded_preds)):\n",
        "#             test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
        "#             test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
        "\n",
        "\n",
        "#     avg_cer = sum(test_cer)/len(test_cer)\n",
        "#     avg_wer = sum(test_wer)/len(test_wer)\n",
        "\n",
        "#     test_losses.append(test_loss/len(drvctk_test_dataset))\n",
        "#     wers_epoch.append(avg_wer)\n",
        "#     cers_epoch.append(avg_cer)\n",
        "\n",
        "#     print('Epoch: {}, Test Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(epoch,test_loss/len(drvctk_test_dataset), avg_cer, avg_wer))\n",
        "\n",
        "#     torch.save(model, '/content/drive/My Drive/DLS_Project/best_deep_s.pt')\n",
        "    \n",
        "#     # increment epoch\n",
        "#     epoch += 1\n",
        "  \n",
        "#   print(\"Training Finished!\")\n",
        "#   Training_Return = (losses,test_losses, wers_epoch, cers_epoch)\n",
        "#   return Training_Return"
      ],
      "metadata": {
        "id": "OusvDQc_wE2x"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Train MNIST with test set as validation\n",
        "# res = train(Training_Result)\n",
        "\n",
        "# losses,test_losses, wers_epoch, cers_epoch = res"
      ],
      "metadata": {
        "id": "RPOEkDOywE5e"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.eval()\n",
        "# test_loss = 0\n",
        "# test_cer, test_wer = [], []\n",
        "# with torch.no_grad():\n",
        "#   for _data in clean_test_loader:\n",
        "\n",
        "#     spectrograms, labels, input_lengths, label_lengths = _data \n",
        "\n",
        "#     spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "#     output = model(spectrograms)  # (batch, time, n_class)\n",
        "#     output = F.log_softmax(output, dim=2)\n",
        "#     output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "\n",
        "#     loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "#     test_loss += loss.item() \n",
        "\n",
        "#     print(output.transpose(0, 1))\n",
        "#     print(labels)\n",
        "#     decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
        "\n",
        "#     for j in range(len(decoded_preds)):\n",
        "#         test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
        "#         test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
        "\n",
        "\n",
        "# avg_cer = sum(test_cer)/len(test_cer)\n",
        "# avg_wer = sum(test_wer)/len(test_wer)\n",
        "\n",
        "# test_losses.append(test_loss/len(drvctk_test_dataset))\n",
        "# wers_epoch.append(avg_wer)\n",
        "# cers_epoch.append(avg_cer)"
      ],
      "metadata": {
        "id": "JRWjaqtCwE70"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sET4QTMhwE-S"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Pkns_qIwFA8"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-a98WxsbwFDU"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2jvpdVz0wFF5"
      },
      "execution_count": 45,
      "outputs": []
    }
  ]
}